# Copie este arquivo para .env e preencha com seus valores

# ===== CONFIGURAÇÃO LLM =====
# Provedor LLM: "gemini" ou "ollama"
LLM_PROVIDER=gemini

# ===== GEMINI (Google) =====
# API do Google Gemini
GOOGLE_API_KEY=sua_api_key_aqui

# ===== OLLAMA (Local) =====
# URL base do Ollama (padrão: http://localhost:11434/v1)
OLLAMA_BASE_URL=http://localhost:11434/v1
# Modelo Ollama a ser usado (padrão: gpt-oss:20b)
OLLAMA_MODEL=gpt-oss:20b
# API Key do Ollama (pode ser qualquer valor, Ollama não valida)
OLLAMA_API_KEY=ollama

# ===== BANCO DE DADOS =====
# Configurações do Banco de Dados PostgreSQL
DB_USER=frontend_user
DB_PASSWORD=senha_forte
DB_HOST=localhost
DB_PORT=5432
DB_NAME=analytics_db

# String de conexão completa (será construída automaticamente)
# PG_CONNECTION_STRING será gerada automaticamente no docker-compose.yml
